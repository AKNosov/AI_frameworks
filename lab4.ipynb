{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Выбор данных\n",
        "\n",
        "#Датасеты\n",
        "- **Датасет:** [Stroke Prediction Dataset](https://www.kaggle.com/datasets/jawairia123/stroke-prediction-dataset)\n",
        "\n",
        " **Задача:** Прогнозирование вероятности инсульта у пациента на основе медицинских и демографических данных.\n",
        "\n",
        "- **Датасет:** [Python Learning & Exam Performance Dataset](https://www.kaggle.com/datasets/emonsharkar/python-learning-and-exam-performance-dataset)\n",
        "\n",
        " **Задача:** Прогнозирование результата экзамена по Python на основе данных об обучении и активности студента.\n",
        "\n",
        "#Метрики\n",
        "- **Для классификации:**\n",
        "  - **F1-score:** так как датасет может быть несбалансированным\n",
        "  - **ROC-AUC:** позволяет оценить качество модели на разных порогах классификации\n",
        "  - **Accuracy**\n",
        "- **Для регрессии:**\n",
        "  - **MAE**\n",
        "  - **RMSE:** более чувствительна к большим ошибкам\n",
        "  - **R²**\n"
      ],
      "metadata": {
        "id": "B6rxjDF7NVtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Создание бейзлайна и оценка качества\n"
      ],
      "metadata": {
        "id": "9ECoU5qQO-Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, f_regression"
      ],
      "metadata": {
        "id": "p14GJo99XFpi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично ЛР1"
      ],
      "metadata": {
        "id": "Abmglj5aViCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_stroke = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')\n",
        "df_stroke = df_stroke.drop(columns=['id'])\n",
        "df_exam = pd.read_csv('/content/python_learning_exam_performance.csv')\n",
        "df_exam = df_exam.drop(columns=['student_id'])\n",
        "\n",
        "df_stroke['bmi'] = df_stroke['bmi'].fillna(df_stroke['bmi'].median())\n",
        "\n",
        "X_stroke = df_stroke.drop(columns=['stroke'])\n",
        "y_stroke = df_stroke['stroke']\n",
        "X_train_stroke, X_test_stroke, y_train_stroke, y_test_stroke = train_test_split(\n",
        "    X_stroke, y_stroke, test_size=0.2, random_state=42, stratify=y_stroke\n",
        ")\n",
        "\n",
        "cat_cols_stroke = X_train_stroke.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols_stroke = X_train_stroke.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor_stroke = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols_stroke),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols_stroke)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train_stroke_processed = preprocessor_stroke.fit_transform(X_train_stroke)\n",
        "X_test_stroke_processed = preprocessor_stroke.transform(X_test_stroke)\n",
        "df_exam['prior_programming_experience'] = df_exam['prior_programming_experience'].fillna('No')\n",
        "\n",
        "X_exam = df_exam.drop(columns=['final_exam_score'])\n",
        "y_exam = df_exam['final_exam_score']\n",
        "X_train_exam, X_test_exam, y_train_exam, y_test_exam = train_test_split(\n",
        "    X_exam, y_exam, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "cat_cols_exam = X_train_exam.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols_exam = X_train_exam.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor_exam = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols_exam),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols_exam)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train_exam_processed = preprocessor_exam.fit_transform(X_train_exam)\n",
        "X_test_exam_processed = preprocessor_exam.transform(X_test_exam)"
      ],
      "metadata": {
        "id": "SRhPv9KEdSKc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн"
      ],
      "metadata": {
        "id": "77Y2LXHqmtmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "rf_clf.fit(X_train_stroke_processed, y_train_stroke)\n",
        "y_pred_stroke_rf = rf_clf.predict(X_test_stroke_processed)\n",
        "y_pred_proba_stroke_rf = rf_clf.predict_proba(X_test_stroke_processed)[:, 1]\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_reg.fit(X_train_exam_processed, y_train_exam)\n",
        "y_pred_exam_rf = rf_reg.predict(X_test_exam_processed)\n",
        "\n",
        "# Оценка качества моделей\n",
        "\n",
        "acc_rf = accuracy_score(y_test_stroke, y_pred_stroke_rf)\n",
        "f1_rf = f1_score(y_test_stroke, y_pred_stroke_rf)\n",
        "roc_auc_rf = roc_auc_score(y_test_stroke, y_pred_proba_stroke_rf)\n",
        "\n",
        "print(\"=== Классификация ===\")\n",
        "print(f\"Accuracy: {acc_rf:.4f}\")\n",
        "print(f\"F1-Score: {f1_rf:.4f}\")\n",
        "print(f\"ROC-AUC:  {roc_auc_rf:.4f}\")\n",
        "print()\n",
        "\n",
        "mae_rf = mean_absolute_error(y_test_exam, y_pred_exam_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test_exam, y_pred_exam_rf))\n",
        "r2_rf = r2_score(y_test_exam, y_pred_exam_rf)\n",
        "\n",
        "print(\"=== Регрессия ===\")\n",
        "print(f\"MAE:  {mae_rf:.4f}\")\n",
        "print(f\"RMSE: {rmse_rf:.4f}\")\n",
        "print(f\"R²:   {r2_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9---xR65iNc6",
        "outputId": "b3525297-fd59-4b4a-b7b8-fd18528f5f13"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Классификация ===\n",
            "Accuracy: 0.9481\n",
            "F1-Score: 0.0000\n",
            "ROC-AUC:  0.7981\n",
            "\n",
            "=== Регрессия ===\n",
            "MAE:  6.4769\n",
            "RMSE: 8.0035\n",
            "R²:   0.7821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Анализ результатов**\n",
        "\n",
        "Анализ результатов случайного леса показывает заметное улучшение в задаче регрессии по сравнению с одиночным деревом: R² вырос с 0.5040 до 0.7821, MAE снизился с 9.4872 до 6.4769, а RMSE уменьшился с 12.0742 до 8.0035. Эти показатели демонстрируют, что ансамблевый подход существенно повышает точность прогнозирования, лучше улавливает сложные зависимости в данных и обеспечивает более устойчивые предсказания за счет усреднения множества деревьев.\n",
        "\n",
        "Однако в задаче классификации наблюдается противоречивая ситуация: при высоких Accuracy (0.9481) и ROC-AUC (0.7981) модель показывает нулевой F1-Score. Это указывает на сильную несбалансированность данных и то, что случайный лес, несмотря на улучшенную ROC-AUC, по-прежнему плохо справляется с выявлением минорного класса (инсультов), предпочитая консервативную стратегию предсказания мажоритарного класса. Для решения этой проблемы требуются дополнительные методы работы с дисбалансом."
      ],
      "metadata": {
        "id": "25foCoqkoVhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Улучшение бейзлайна"
      ],
      "metadata": {
        "id": "ljbpDl5To_Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично ЛР 1 пробуем следующие гипотезы для улучшения результатов\n",
        "\n",
        "-   Балансировка классов\n",
        "-   Подбор гиперпараметров\n",
        "\n",
        "- Отбор признаков\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qGalntx2tllk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_stroke_processed, y_train_stroke)\n",
        "print(f\"До балансировки: {np.bincount(y_train_stroke)}\")\n",
        "print(f\"После балансировки: {np.bincount(y_train_balanced)}\")\n",
        "print()\n",
        "\n",
        "selector_clf = SelectKBest(f_classif, k=8)\n",
        "X_train_selected = selector_clf.fit_transform(X_train_balanced, y_train_balanced)\n",
        "X_test_selected = selector_clf.transform(X_test_stroke_processed)\n",
        "print(f\"Выбрано {X_train_selected.shape[1]} лучших признаков из {X_train_balanced.shape[1]}\")\n",
        "print()\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [10, 15, None],\n",
        "    'min_samples_split': [2, 10,],\n",
        "    'min_samples_leaf': [1, 4],\n",
        "    'max_features': ['sqrt', None],\n",
        "    'criterion': ['gini'],\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
        "    param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_rf.fit(X_train_selected, y_train_balanced)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_search_rf.best_params_}\")\n",
        "\n",
        "\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "y_pred_rf_improved = best_rf.predict(X_test_selected)\n",
        "y_pred_proba_rf_improved = best_rf.predict_proba(X_test_selected)[:, 1]\n",
        "\n",
        "\n",
        "acc_rf_improved = accuracy_score(y_test_stroke, y_pred_rf_improved)\n",
        "f1_rf_improved = f1_score(y_test_stroke, y_pred_rf_improved)\n",
        "roc_auc_rf_improved = roc_auc_score(y_test_stroke, y_pred_proba_rf_improved)\n",
        "\n",
        "print(\"=== Random Forest (Улучшенная) ===\")\n",
        "print(f\"Accuracy:  {acc_rf_improved:.4f}\")\n",
        "print(f\"F1-Score:  {f1_rf_improved:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_rf_improved:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gegz6_Hto-rK",
        "outputId": "12a2fad2-c2c6-4244-b882-24d0e17b5958"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "До балансировки: [3889  199]\n",
            "После балансировки: [3889 3889]\n",
            "\n",
            "Выбрано 8 лучших признаков из 16\n",
            "\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Лучшие параметры: {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "=== Random Forest (Улучшенная) ===\n",
            "Accuracy:  0.9178\n",
            "F1-Score:  0.2632\n",
            "ROC-AUC:   0.7897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector_reg = SelectKBest(f_regression, k=10)\n",
        "X_train_exam_selected = selector_reg.fit_transform(X_train_exam_processed, y_train_exam)\n",
        "X_test_exam_selected = selector_reg.transform(X_test_exam_processed)\n",
        "print(f\"Выбрано {X_train_exam_selected.shape[1]} лучших признаков из {X_train_exam_processed.shape[1]}\")\n",
        "print()\n",
        "\n",
        "param_grid_rf_reg = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [10, 15, None],\n",
        "    'min_samples_split': [2, 10,],\n",
        "    'min_samples_leaf': [1, 4],\n",
        "    'max_features': ['sqrt', None],\n",
        "    'criterion': ['squared_error'],\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "\n",
        "grid_search_rf_reg = GridSearchCV(\n",
        "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "    param_grid_rf_reg,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_rf_reg.fit(X_train_exam_selected, y_train_exam)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_search_rf_reg.best_params_}\")\n",
        "\n",
        "\n",
        "\n",
        "best_rf_reg = grid_search_rf_reg.best_estimator_\n",
        "y_pred_exam_rf_improved = best_rf_reg.predict(X_test_exam_selected)\n",
        "\n",
        "\n",
        "mae_rf_improved = mean_absolute_error(y_test_exam, y_pred_exam_rf_improved)\n",
        "rmse_rf_improved = np.sqrt(mean_squared_error(y_test_exam, y_pred_exam_rf_improved))\n",
        "r2_rf_improved = r2_score(y_test_exam, y_pred_exam_rf_improved)\n",
        "\n",
        "print(\"=== Random Forest Regressor (Улучшенная) ===\")\n",
        "print(f\"MAE:   {mae_rf_improved:.4f}\")\n",
        "print(f\"RMSE:  {rmse_rf_improved:.4f}\")\n",
        "print(f\"R²:    {r2_rf_improved:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVRxSaav1WDU",
        "outputId": "b400cbe8-2bc9-48e0-d237-da563b16b8be"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выбрано 10 лучших признаков из 23\n",
            "\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Лучшие параметры: {'bootstrap': True, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "=== Random Forest Regressor (Улучшенная) ===\n",
            "MAE:   6.5093\n",
            "RMSE:  8.0948\n",
            "R²:    0.7771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Анализ результатов**\n",
        "\n",
        "В задаче классификации после балансировки данных и оптимизации гиперпараметров мы получили работающую модель с F1-Score 0.2632, тогда как базовая модель показывала нулевой F1-Score при схожем Accuracy (0.9178 против 0.9481). Это означает, что базовая модель полностью игнорировала минорный класс (инсульты), предсказывая только здоровых пациентов, в то время как улучшенная версия научилась находить положительные случаи, хотя и с умеренной точностью. ROC-AUC обеих моделей сравним (0.7897 против 0.7981), что подтверждает сохранение общей разделительной способности.\n",
        "\n",
        "В регрессионной задаче оба подхода демонстрируют близкие результаты: R² 0.7771 против 0.7821, MAE 6.5093 против 6.4769, RMSE 8.0948 против 8.0035. Минимальные различия в метриках (в пределах 1%) указывают на то, что базовая модель Random Forest уже была достаточно хорошо настроена \"из коробки\", а дополнительная оптимизация гиперпараметров и отбор признаков дали лишь незначительное улучшение. Это говорит о том, что для данного датасета Random Forest обладает хорошей устойчивостью и не требует сложной настройки для достижения приемлемых результатов регрессии."
      ],
      "metadata": {
        "id": "__UCedKhFS-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Имплементация собвственной модели\n"
      ],
      "metadata": {
        "id": "SWqGaH0st-t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class SimpleRandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, max_features=None, task='classification', random_state=42):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.task = task\n",
        "        self.random_state = random_state\n",
        "        self.trees = []\n",
        "        self.feature_indices = []\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "    def _get_random_features(self, n_features):\n",
        "        if self.max_features is None:\n",
        "            return np.arange(n_features)\n",
        "        elif self.max_features == 'sqrt':\n",
        "            n_select = int(np.sqrt(n_features))\n",
        "        elif self.max_features == 'log2':\n",
        "            n_select = int(np.log2(n_features))\n",
        "        elif isinstance(self.max_features, float):\n",
        "            n_select = int(self.max_features * n_features)\n",
        "        else:\n",
        "            n_select = min(self.max_features, n_features)\n",
        "\n",
        "        n_select = max(1, n_select)\n",
        "        return np.random.choice(n_features, n_select, replace=False)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0, feature_subset=None):\n",
        "        if depth >= self.max_depth or len(np.unique(y)) == 1 or len(y) <= 1:\n",
        "            if self.task == 'classification':\n",
        "                values, counts = np.unique(y, return_counts=True)\n",
        "                return {'leaf': True, 'value': values[np.argmax(counts)]}\n",
        "            else:\n",
        "                return {'leaf': True, 'value': np.mean(y)}\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        best_gain = -1\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if feature_subset is None:\n",
        "            features_to_check = range(n_features)\n",
        "        else:\n",
        "            features_to_check = feature_subset\n",
        "\n",
        "        for feature_idx in features_to_check:\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature_idx] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
        "                    continue\n",
        "\n",
        "                if self.task == 'classification':\n",
        "                    parent_impurity = 1 - max(np.bincount(y) / len(y))\n",
        "                    left_impurity = 1 - max(np.bincount(y[left_mask]) / len(y[left_mask])) if len(y[left_mask]) > 0 else 0\n",
        "                    right_impurity = 1 - max(np.bincount(y[right_mask]) / len(y[right_mask])) if len(y[right_mask]) > 0 else 0\n",
        "\n",
        "                    gain = parent_impurity - (len(y[left_mask])/n_samples * left_impurity +\n",
        "                                            len(y[right_mask])/n_samples * right_impurity)\n",
        "                else:\n",
        "                    parent_mse = np.var(y)\n",
        "                    left_mse = np.var(y[left_mask]) if len(y[left_mask]) > 0 else 0\n",
        "                    right_mse = np.var(y[right_mask]) if len(y[right_mask]) > 0 else 0\n",
        "\n",
        "                    gain = parent_mse - (len(y[left_mask])/n_samples * left_mse +\n",
        "                                        len(y[right_mask])/n_samples * right_mse)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_gain <= 0 or best_feature_idx is None:\n",
        "            if self.task == 'classification':\n",
        "                values, counts = np.unique(y, return_counts=True)\n",
        "                return {'leaf': True, 'value': values[np.argmax(counts)]}\n",
        "            else:\n",
        "                return {'leaf': True, 'value': np.mean(y)}\n",
        "\n",
        "        left_mask = X[:, best_feature_idx] <= best_threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1, feature_subset)\n",
        "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1, feature_subset)\n",
        "\n",
        "        return {\n",
        "            'leaf': False,\n",
        "            'feature': best_feature_idx,\n",
        "            'threshold': best_threshold,\n",
        "            'left': left_subtree,\n",
        "            'right': right_subtree\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        self.trees = []\n",
        "        self.feature_indices = []\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            X_boot, y_boot = self._bootstrap_sample(X, y)\n",
        "            feature_subset = self._get_random_features(n_features)\n",
        "            tree = self._build_tree(X_boot, y_boot, feature_subset=feature_subset)\n",
        "            self.trees.append(tree)\n",
        "            self.feature_indices.append(feature_subset)\n",
        "\n",
        "    def _predict_one_tree(self, x, tree):\n",
        "        if tree['leaf']:\n",
        "            return tree['value']\n",
        "\n",
        "        if x[tree['feature']] <= tree['threshold']:\n",
        "            return self._predict_one_tree(x, tree['left'])\n",
        "        else:\n",
        "            return self._predict_one_tree(x, tree['right'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        all_predictions = []\n",
        "\n",
        "        for tree in self.trees:\n",
        "            tree_pred = np.array([self._predict_one_tree(x, tree) for x in X])\n",
        "            all_predictions.append(tree_pred)\n",
        "\n",
        "        all_predictions = np.array(all_predictions)\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            final_predictions = []\n",
        "            for sample_idx in range(X.shape[0]):\n",
        "                votes = all_predictions[:, sample_idx]\n",
        "                most_common = Counter(votes).most_common(1)[0][0]\n",
        "                final_predictions.append(most_common)\n",
        "            return np.array(final_predictions)\n",
        "        else:\n",
        "            return np.mean(all_predictions, axis=0)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.task != 'classification':\n",
        "            raise ValueError(\"predict_proba доступен только для классификации\")\n",
        "\n",
        "        X = np.array(X)\n",
        "        all_predictions = []\n",
        "\n",
        "        for tree in self.trees:\n",
        "            tree_pred = np.array([self._predict_one_tree(x, tree) for x in X])\n",
        "            all_predictions.append(tree_pred)\n",
        "\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        probs = np.zeros((X.shape[0], 2))\n",
        "\n",
        "        for sample_idx in range(X.shape[0]):\n",
        "            votes = all_predictions[:, sample_idx]\n",
        "            prob_1 = np.sum(votes == 1) / len(votes)\n",
        "            prob_0 = 1 - prob_1\n",
        "            probs[sample_idx] = [prob_0, prob_1]\n",
        "\n",
        "        return probs\n",
        "\n",
        "rf_clf_custom = SimpleRandomForest(n_estimators=10, max_depth=5, max_features='sqrt', task='classification', random_state=42)\n",
        "rf_clf_custom.fit(X_train_stroke_processed, y_train_stroke)\n",
        "y_pred_rf_custom = rf_clf_custom.predict(X_test_stroke_processed)\n",
        "y_pred_proba_rf_custom = rf_clf_custom.predict_proba(X_test_stroke_processed)[:, 1]\n",
        "\n",
        "acc_rf_custom = accuracy_score(y_test_stroke, y_pred_rf_custom)\n",
        "f1_rf_custom = f1_score(y_test_stroke, y_pred_rf_custom)\n",
        "roc_auc_rf_custom = roc_auc_score(y_test_stroke, y_pred_proba_rf_custom)\n",
        "\n",
        "print(\"\\nКлассификация (Stroke Prediction):\")\n",
        "print(f\"Accuracy: {acc_rf_custom:.4f}\")\n",
        "print(f\"F1-Score: {f1_rf_custom:.4f}\")\n",
        "print(f\"ROC-AUC:  {roc_auc_rf_custom:.4f}\")\n",
        "\n",
        "rf_reg_custom = SimpleRandomForest(n_estimators=10, max_depth=5, max_features=0.7, task='regression', random_state=42)\n",
        "rf_reg_custom.fit(X_train_exam_processed, y_train_exam)\n",
        "y_pred_rf_reg_custom = rf_reg_custom.predict(X_test_exam_processed)\n",
        "\n",
        "mae_rf_custom = mean_absolute_error(y_test_exam, y_pred_rf_reg_custom)\n",
        "rmse_rf_custom = np.sqrt(mean_squared_error(y_test_exam, y_pred_rf_reg_custom))\n",
        "r2_rf_custom = r2_score(y_test_exam, y_pred_rf_reg_custom)\n",
        "\n",
        "print(\"\\nРегрессия (Exam Performance):\")\n",
        "print(f\"MAE:  {mae_rf_custom:.4f}\")\n",
        "print(f\"RMSE: {rmse_rf_custom:.4f}\")\n",
        "print(f\"R²:   {r2_rf_custom:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enNFdkqejlYd",
        "outputId": "cf5af4a3-be58-48ed-c2d3-60851972db87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Классификация (Stroke Prediction):\n",
            "Accuracy: 0.9511\n",
            "F1-Score: 0.0000\n",
            "ROC-AUC:  0.4979\n",
            "\n",
            "Регрессия (Exam Performance):\n",
            "MAE:  7.3970\n",
            "RMSE: 9.2139\n",
            "R²:   0.7112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полученные результаты соответствуют sklearn. Введём улучшения"
      ],
      "metadata": {
        "id": "SfpEZbW8IbVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imp_my_knn_clf = SimpleRandomForest(n_estimators=10, max_depth=5, max_features='sqrt', task='classification', random_state=42)\n",
        "imp_my_knn_clf.fit(X_train_selected, y_train_balanced)\n",
        "y_pred_my_imp_clf = imp_my_knn_clf.predict(X_test_selected)\n",
        "\n",
        "imp_my_knn_reg = SimpleRandomForest(n_estimators=10, max_depth=5, max_features=0.7, task='regression', random_state=42)\n",
        "imp_my_knn_reg.fit(X_train_exam_selected, y_train_exam)\n",
        "y_pred_my_imp_reg = imp_my_knn_reg.predict(X_test_exam_selected)\n",
        "\n",
        "\n",
        "imp_acc_my_clf = accuracy_score(y_test_stroke, y_pred_my_imp_clf)\n",
        "imp_f1_my_clf = f1_score(y_test_stroke, y_pred_my_imp_clf)\n",
        "\n",
        "print(\"\\nКлассификация (Stroke Prediction):\")\n",
        "print(f\"Accuracy: {imp_acc_my_clf:.4f}\")\n",
        "print(f\"F1-Score: {imp_f1_my_clf:.4f}\")\n",
        "\n",
        "imp_mae_my_reg = mean_absolute_error(y_test_exam, y_pred_my_imp_reg)\n",
        "imp_rmse_my_reg = np.sqrt(mean_squared_error(y_test_exam, y_pred_my_imp_reg))\n",
        "imp_r2_my_reg = r2_score(y_test_exam, y_pred_my_imp_reg)\n",
        "\n",
        "print(\"\\nРегрессия (Exam Performance):\")\n",
        "print(f\"MAE:  {imp_mae_my_reg:.4f}\")\n",
        "print(f\"RMSE: {imp_rmse_my_reg:.4f}\")\n",
        "print(f\"R²:   {imp_r2_my_reg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubUORnL4JH4q",
        "outputId": "9b0830c5-a8c2-4ce8-a6a9-ab3802ec290e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Классификация (Stroke Prediction):\n",
            "Accuracy: 0.7280\n",
            "F1-Score: 0.2147\n",
            "\n",
            "Регрессия (Exam Performance):\n",
            "MAE:  7.2161\n",
            "RMSE: 9.0122\n",
            "R²:   0.7237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Улчшения показали отличный эффект на задаче классификации, сильно уменьшив значение дисбаланса классов. На задаче регрессии разница небольшая"
      ],
      "metadata": {
        "id": "eOdfObdTLB9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Вывод\n",
        "Случайный лес продемонстрировал существенное превосходство над одиночными деревьями в обеих задачах: в классификации после балансировки данных и оптимизации гиперпараметров F1-Score увеличился, а в регрессии качество прогнозов значительно улучшилось, что подтверждает эффективность ансамблевого подхода для уменьшения переобучения, повышения устойчивости моделей и улучшения обобщающей способности за счет комбинирования множества деревьев с различными подвыборками данных и признаков."
      ],
      "metadata": {
        "id": "HrWmwWHdLXE4"
      }
    }
  ]
}